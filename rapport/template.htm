<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="fr">
<head>
  <title>On Spectral Clustering: Analysis and an algorithm</title>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link rel="stylesheet" href="./style.css" type="text/css">
  <meta name="KeyWords" content="ISIMA, TP, imagerie">
  <meta name="Description" content="Compte-rendu de projet de TP d'Algorithmes pour le traitement d'images">
</head>

<body>
  <h1>On Spectral Clustering: Analysis and an algorithm</h1>
  <h3>Benoît Garçon & Pierre-Loup Pissavy, PROMO 17 F2</h3>
  <h1 class="titrearticle"> Pr&eacute;sentation du sujet </h1>
  <p>
    Ce rapport a été rédigé dans le cadre d’un cours d’algorithme pour le traitement d'image délivré par V. Barra et C. Tilmant, suivi à l’Institut Supérieur d’Informatique, de Modélisation et de leurs Applications (ISIMA). Il a pour but de présenter succinctement nos travaux sur le programme de clustering d'images basé sur de l'analyse spectrale. Ce document est un complément du programme implémenté en C++ et dont les aspects sont abordés plus loin.
  </p>
  <p>
    Au cours de ce projet d'imagerie nous avons donc réussi à développer un programme paramétrable permettant d'extraire dans une images N ensembles de points ou zones à similarité élevée. Ce programme se base sur les travaux d'Andrew Y. Ng, Michael I. Jordan et Yair Weiss sur le clustering spectral en 2002 <strong>[1]</strong>.
	</p>
	<p>
		L'idée générale de leur algorithme est assez simple : le coeur du travail consiste à partitionner l'image avec un algorithme classique des K-Means mais au lieu de baser la classification directement sur les paramètres des points, on effectue d'abord une analyse spectrale des points. Ainsi on s'abstrait d'un côté de l'espace des paramètres et on peut détecter des zones dites <strong>non-convexes</strong> là où un simple K-Means se limiterait à des formes convexes.
  </p>
  <p>
    Nous présenterons tout d’abord l’algorithme, son principe et les détails de son implémentation. Ensuite nous exposerons les différents résultats obtenus grâce au programme produit. Enfin nous discuterons des différents résultats par rapport aux objectifs initiaux.
  </p>
  <br/>
  <h1 class="titrearticle"> M&eacute;thode</h1>


  <h2 class="soustitrearticle"> Partie th&eacute;orique</h2>
	<p>
		Pr&eacute;sentation de la m&eacute;thode utilis&eacute;e et r&eacute;f&eacute;rence aux travaux connexes (cf. biblio article)

		<ol class="algo">
			<h3><li>
				Préparation des données 
			</li></h3>
			<p>
				
			</p>
			<h3><li>
				Calcul de la matrice d'affinité A
			</li></h3>
			<p>
				
			</p>
			<h3><li>
				Calcul de la matrice d'analyse L
			</li></h3>
			<p>
				
			</p>
			<h3><li>
				Extraction des k vecteurs propres ayant les plus grandes valeurs propres
			</li></h3>
			<p>
				
			</p>
			<h3><li>
				Renormalisation de la matrice des k vecteurs propres
			</li></h3>
			<p>
				
			</p>
			<h3><li>
				Application d'un algorithme de clustering sur la matrice des vecteurs propres
			</li></h3>
			<p>
				
			</p>
			<h3><li>
				Assignement des points au cluster de leur ligne de la matrice des vecteurs propres
			</li></h3>
			<p>
				
			</p>
		</ol>
	</p>
  <h2 class="soustitrearticle"> Implémentation</h2>
	<p>
		Nous allons maintenant aborder les détails d'implémentation de notre programme. Celui-ci se présente sous la forme d'un code C++ utilisant la bibliothèque CImg pour le traitement des images. Son utilisation est assez simple :
	</p>	
	<ul>
		<em>spectral_clustering [ input_image [ nb_classes [ sigma ] ] ]</em> avec
		<li>
			<strong>input_image</strong> : le chemin vers l'image à traiter
		</li>
		<li>
			<strong>nb_classes</strong> : le nombre de classes k que l'on veut obtenir
		</li>
		<li>
			<strong>sigma</strong> : le paramètre de controle de la plongée de la matrice d'affinité
		</li>
	</ul>
	<p>
		Le code source se décompose en trois classes très liées mais qui permettent une grande modularité du code :
	</p>
	<ul>
		<li>
			<strong>ClusteringAlgorithm</strong> : cette classe abstraite propose une interface générique de clustering et permet de modulariser le code pour pouvoir facilement interchanger les algorithmes avec des foncteurs.
		</li>
		<li>
			<strong>SpectralAlgorithm</strong> : cette classe implémente sa classe mère ClusteringAlgorithm avec l'algorithme vu précédemment. L'algorithme de classement de l'étape 5 peut-être modifier grâce au polymorphisme du foncteur utilisé.
		</li>
		<li>
			<strong>KMeans</strong> : cette classe implémente sa classe mère ClusteringAlgorithm avec l'algorithme des K-Means.
		</li>
	</ul>
	<p>
		<ol class="algo">
			<h3><li>
				Préparation des données 
			</li></h3>
			<p>
				
			</p>
			<h3><li>
				Calcul de la matrice d'affinité A
			</li></h3>
			<p>
				
			</p>
			<h3><li>
				Calcul de la matrice d'analyse L
			</li></h3>
			<p>
				
			</p>
			<h3><li>
				Extraction des k vecteurs propres ayant les plus grandes valeurs propres
			</li></h3>
			<p>
				
			</p>
			<h3><li>
				Renormalisation de la matrice des k vecteurs propres
			</li></h3>
			<p>
				
			</p>
			<h3><li>
				Application d'un algorithme de clustering sur la matrice des vecteurs propres
			</li></h3>
			<p>
				
			</p>
			<h3><li>
				Assignement des points au cluster de leur ligne de la matrice des vecteurs propres
			</li></h3>
			<p>
				
			</p>
		</ol>
	</p>
  <br/>
  <h1 class="titrearticle"> R&eacute;sultats</h1>

  <h2 class="soustitrearticle"> Validation sur donn&eacute;es de synth&egrave;se</h2>
	<p>
	  Vous devez cr&eacute;er des donn&eacute;es de synth&egrave;se pour &eacute;valuer votre m&eacute;thode
	</p>
  <h2 class="soustitrearticle"> Donn&eacute;es r&eacute;elles</h2>
  <p>
  	Une fois la m&eacute;thode &eacute;valu&eacute;e, vous appliquerez votre m&eacute;thode sur des images r&eacute;elles.
	</p>
	<br/>
  <h1 class="titrearticle"> Discussion</h1>
	<p>
	  Commentez vos r&eacute;sultats (d'un point de vue qualitatif <span style="font-weight: bold;">ET</span> quantitatif)
  </p>
  <br/>
  <h1 class="titrearticle"> R&eacute;f&eacute;rences</h1>
  <p>
	<ul class="references">
	  <li>[1] Andrew Y Ng, Michael I Jordan, and Yair Weiss. On Spectral Clustering: Analysis and an algorithm. Advances in Neural Information Processing Systems, pages 849-856, 2001.
	  </li><li>[2] Charles J. Alpert , Andrew B. Kahng , So-Zen Yao, Spectral partitioning with multiple eigenvectors, Discrete Applied Mathematics, v.90 n.1-3, p.3-26, Jan. 15, 1999
	  </li><li>[3] N. Christianini, J. Shawe-Taylor, and J. Kandola. Spectral kernel methods for clustering. In Neural Information Processing Systems 14, 2002.
	  </li><li>[4] F. Chung. Spectral Graph Theory. Number 92 in CBMS Regional Conference Series in Mathematics. American Mathematical Society, 1997.
	  </li><li>[5] R. Kannan , S. Vempala , A. Veta, On clusterings-good, bad and spectral, Proceedings of the 41st Annual Symposium on Foundations of Computer Science, p.367, November 12-14, 2000
	  </li><li>[6] J. Malik, S. Belongie, T. Leung, and J. Shi. Contour and texture analysis for image segmentation. In Perceptual Organization for Artificial Vision Systems. Kluwcr, 2000,
	  </li><li>[7] M. Meila and J. Shi. Learning segmentation by random walks. In Neural Information Processing Systems 13. 2001.
	  </li><li>[8] Bernhard Schölkopf , Alexander Smola , Klaus-Robert Müller, Nonlinear component analysis as a kernel eigenvalue problem, Neural Computation, v.10 n.5, p.1299-1319, July 1, 1998
	  </li><li>[9] G. Scott and H. Longuet-Higgins. Feature grouping by relocalisation of eigenvectors of the proximity matrix. In Proc. British Machine Vision Conference, 1990.
	  </li><li>[10] D. A. Spielmat, Spectral partitioning works: planar graphs and finite element meshes, Proceedings of the 37th Annual Symposium on Foundations of Computer Science, p.96, October 14-16, 1996
	  </li><li>[11] G. W. Stewart and J.-G. Sun. Matrix Perturbation Theory. Academic Press, 1990.
	  </li><li>[12] Yair Weiss, Segmentation Using Eigenvectors: A Unifying View, Proceedings of the International Conference on Computer Vision-Volume 2, p.975, September 20-25, 1999 
		</li>
	</ul>
	</p>
</body>
</html>
